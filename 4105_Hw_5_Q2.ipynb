{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5uWyF92v0mI6iMEoNvhzk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pathin220/4105_ML_Hw5/blob/main/4105_Hw_5_Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.a. Develop preprocessing and a training loop to train a linear regression model that predicts housing price based on the following input variables:\n",
        "#area, bedrooms, bathrooms, stories, parking\n",
        "\n",
        "#For this, you need to use the housing dataset. For training and validation, use 80% (training) and 20% (validation) split.\n",
        "# Identify the best parameters for your linear regression model based on the above input variables. In this case, you will have six parameters:\n",
        "\n",
        "#2.b Use 5000 epochs for your training. Explore different learning rates from 0.1 to 0.0001 (you need four separate trainings).\n",
        "# Report your loss and validation accuracy for every 500 epochs per training. Pick the best linear model.\n",
        "\n",
        "#2.c. Compare your results against the linear regression done in homework 1. Do you see meaningful differences?"
      ],
      "metadata": {
        "id": "nNliBTF-L-rQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "ay31g1l6MOL6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/My Drive/Machine Learning/Housing.csv'\n",
        "housing = pd.DataFrame(pd.read_csv(file_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnB_XUWeL_D_",
        "outputId": "cc42a6bd-994d-4479-f506-1884efc3aa41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mapping the variables to a binary output\n",
        "varlist_1 =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishingstatus']\n",
        "\n",
        "# Defining the map function\n",
        "def mapping(x):\n",
        "    return x.map({'yes': 1, 'no': 0, 'furnished':  1, 'semi-furnished':  0, 'unfurnished':  -1})\n",
        "\n",
        "housing[varlist_1] = housing[varlist_1].apply(mapping)\n"
      ],
      "metadata": {
        "id": "VcvbE3AhMBlI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the Dataset\n",
        "X_train, X_val = train_test_split(housing, train_size = 0.8, test_size = 0.2, random_state = 0)\n"
      ],
      "metadata": {
        "id": "lflNgeJhMS3k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting Variables\n",
        "Y_train = X_train.pop(\"price\")\n",
        "Y_val = X_val.pop(\"price\")"
      ],
      "metadata": {
        "id": "oKGOCEGAMUdQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardizing the Dataset\n",
        "standard = StandardScaler()\n",
        "vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
        "\n",
        "X_train_st = X_train\n",
        "X_train_st[vars] = standard.fit_transform(X_train_st[vars])\n",
        "\n",
        "X_val_st = X_val\n",
        "X_val_st[vars] = standard.fit_transform(X_val_st[vars])"
      ],
      "metadata": {
        "id": "7abjXlfWMV9z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the loss function\n",
        "def loss_fn(t_p, t_c):\n",
        "    squared_diffs = (t_p - t_c)**2\n",
        "    return squared_diffs.mean()"
      ],
      "metadata": {
        "id": "BXpAhhx5MXjS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the model\n",
        "def model_h(t_u, w5, w4, w3, w2, w1, b):\n",
        "  return w5*t_u**5 + w4*t_u**4 + w3*t_u**3 + w2 * t_u ** 2 + w1 * t_u + b"
      ],
      "metadata": {
        "id": "vZv4ZbKNMY3e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)"
      ],
      "metadata": {
        "id": "cx-XLzs7Mac9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_st_numpy = np.c_[np.ones((len(Y_train), 1)), X_train_st[vars]]\n",
        "X_val_st_numpy = np.c_[np.ones((len(Y_val), 1)), X_val_st[vars]]"
      ],
      "metadata": {
        "id": "xEgbGE4_Mb2k"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_t = torch.tensor(X_train_st_numpy)\n",
        "X_val_t = torch.tensor(X_val_st_numpy)\n",
        "Y_train_t = torch.tensor(Y_train.values)\n",
        "Y_val_t = torch.tensor(Y_val.values)"
      ],
      "metadata": {
        "id": "6kQzlAebMdSb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_m = Y_train_t.float().mean()\n",
        "Y_train_st = Y_train.std()\n",
        "Y_train_st_t = (Y_train_t-Y_train_m) / Y_train_st"
      ],
      "metadata": {
        "id": "hvl-0da0MevD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a training loop\n",
        "def training_loop(n_epochs, optimizer, params, t_u_train, t_c_train, t_u_val, t_c_val):\n",
        "  for epoch in range (1, n_epochs +1):\n",
        "\n",
        "    if params.grad is not None:\n",
        "       params.grad.zero_()\n",
        "\n",
        "    t_p_train = model_h(t_u_train, *params)\n",
        "    loss_train = loss_fn(t_p_train.transpose(0,1), t_c_train)\n",
        "\n",
        "    t_p_val = model_h(t_u_val, *params)\n",
        "    loss_val = loss_fn(t_p_val.transpose(0,1), t_c_val)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if epoch <= 3 or epoch % 500 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss: {loss_train.item():.4f},\"\n",
        "                  f\" Validation loss: {loss_val.item():.4f}\\n\")\n",
        "  return params\n",
        "\n"
      ],
      "metadata": {
        "id": "1P_duKt9MgIb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SGD 1\n",
        "SGD_1_learning_rate = 1e-10\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "optimizer_SGD2 = optim.SGD([params], lr= SGD_1_learning_rate)\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer_SGD2,\n",
        "    params = params,\n",
        "    t_u_train = X_train_t,\n",
        "    t_c_train = Y_train_t,\n",
        "    t_u_val = X_val_t,\n",
        "    t_c_val = Y_val_t,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrNTfYFdMhXO",
        "outputId": "be6b5754-cf0d-4ab6-fd1a-a64011707b98"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss: 26469777039121.0156, Validation loss: 25188979008958.8125\n",
            "\n",
            "Epoch 2, Training loss: 26469775109834.8945, Validation loss: 25188976396646.2383\n",
            "\n",
            "Epoch 3, Training loss: 26469773180553.0859, Validation loss: 25188973784338.6445\n",
            "\n",
            "Epoch 500, Training loss: 26468815393815.4961, Validation loss: 25187676811293.6172\n",
            "\n",
            "Epoch 1000, Training loss: 26467853967709.3555, Validation loss: 25186374708878.4023\n",
            "\n",
            "Epoch 1500, Training loss: 26466894685737.6445, Validation loss: 25185075308681.8281\n",
            "\n",
            "Epoch 2000, Training loss: 26465937542263.7617, Validation loss: 25183778604007.8789\n",
            "\n",
            "Epoch 2500, Training loss: 26464982533492.0234, Validation loss: 25182484590249.2070\n",
            "\n",
            "Epoch 3000, Training loss: 26464029653790.8789, Validation loss: 25181193260458.2383\n",
            "\n",
            "Epoch 3500, Training loss: 26463078901346.4336, Validation loss: 25179904612739.5859\n",
            "\n",
            "Epoch 4000, Training loss: 26462130262689.1289, Validation loss: 25178618630664.7695\n",
            "\n",
            "Epoch 4500, Training loss: 26461183740208.0781, Validation loss: 25177335316884.2969\n",
            "\n",
            "Epoch 5000, Training loss: 26460239348036.9414, Validation loss: 25176054692565.1484\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([65.5474, 23.2831,  8.2987,  6.3373,  2.4985,  4.7773],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SGD 2\n",
        "SGD_2_learning_rate = 1e-15\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "SGD2_optimizer = optim.SGD([params], lr=SGD_2_learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = SGD2_optimizer,\n",
        "    params = params,\n",
        "    t_u_train = X_train_t,\n",
        "    t_c_train = Y_train_t,\n",
        "    t_u_val = X_val_t,\n",
        "    t_c_val = Y_val_t,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irZVU53yMix8",
        "outputId": "d2557f1e-8f50-4605-b836-c635018cfdfd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss: 26469777039121.0156, Validation loss: 25188979008958.8125\n",
            "\n",
            "Epoch 2, Training loss: 26469777039105.4453, Validation loss: 25188979008937.4609\n",
            "\n",
            "Epoch 3, Training loss: 26469777039089.8711, Validation loss: 25188979008916.1055\n",
            "\n",
            "Epoch 500, Training loss: 26469777031350.7656, Validation loss: 25188978998304.1562\n",
            "\n",
            "Epoch 1000, Training loss: 26469777023564.9492, Validation loss: 25188978987628.1445\n",
            "\n",
            "Epoch 1500, Training loss: 26469777015779.1289, Validation loss: 25188978976952.1328\n",
            "\n",
            "Epoch 2000, Training loss: 26469777007993.3125, Validation loss: 25188978966276.1289\n",
            "\n",
            "Epoch 2500, Training loss: 26469777000207.4883, Validation loss: 25188978955600.1211\n",
            "\n",
            "Epoch 3000, Training loss: 26469776992421.6758, Validation loss: 25188978944924.1094\n",
            "\n",
            "Epoch 3500, Training loss: 26469776984635.8516, Validation loss: 25188978934248.0977\n",
            "\n",
            "Epoch 4000, Training loss: 26469776976850.0234, Validation loss: 25188978923572.0781\n",
            "\n",
            "Epoch 4500, Training loss: 26469776969064.1953, Validation loss: 25188978912896.0625\n",
            "\n",
            "Epoch 5000, Training loss: 26469776961278.3711, Validation loss: 25188978902220.0469\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0006e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 4.7779e-05],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SGD 3\n",
        "SGD_3_learning_rate = 1e-20\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "SGD3_optimizer = optim.SGD([params], lr=SGD_3_learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = SGD3_optimizer,\n",
        "    params = params,\n",
        "    t_u_train = X_train_t,\n",
        "    t_c_train = Y_train_t,\n",
        "    t_u_val = X_val_t,\n",
        "    t_c_val = Y_val_t,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkDA6QAcMkZ7",
        "outputId": "a35c7385-79e6-4f6b-c4ef-af053b30b95d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss: 26469777039121.0156, Validation loss: 25188979008958.8125\n",
            "\n",
            "Epoch 2, Training loss: 26469777039121.0156, Validation loss: 25188979008958.8125\n",
            "\n",
            "Epoch 3, Training loss: 26469777039121.0156, Validation loss: 25188979008958.8125\n",
            "\n",
            "Epoch 500, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8125\n",
            "\n",
            "Epoch 1000, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 1500, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 2000, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 2500, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 3000, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 3500, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 4000, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 4500, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 5000, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 4.7778e-10],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SGD 4\n",
        "SGD_4_learning_rate = 1e-25\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "SGD4_optimizer = optim.SGD([params], lr=SGD_3_learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = SGD4_optimizer,\n",
        "    params = params,\n",
        "    t_u_train = X_train_t,\n",
        "    t_c_train = Y_train_t,\n",
        "    t_u_val = X_val_t,\n",
        "    t_c_val = Y_val_t,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTKxGpMdMmE0",
        "outputId": "c37eb453-573e-4e27-fa2b-380efd271f96"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss: 26469777039121.0156, Validation loss: 25188979008958.8125\n",
            "\n",
            "Epoch 2, Training loss: 26469777039121.0156, Validation loss: 25188979008958.8125\n",
            "\n",
            "Epoch 3, Training loss: 26469777039121.0156, Validation loss: 25188979008958.8125\n",
            "\n",
            "Epoch 500, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8125\n",
            "\n",
            "Epoch 1000, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 1500, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 2000, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 2500, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 3000, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 3500, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 4000, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 4500, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n",
            "Epoch 5000, Training loss: 26469777039121.0117, Validation loss: 25188979008958.8086\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 4.7778e-10],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adam Number 1 learning rate of 0.1\n",
        "Adam1_learning_rate = 0.1\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "Adam1_optimizer = optim.Adam([params], lr=Adam1_learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = Adam1_optimizer,\n",
        "    params = params,\n",
        "    t_u_train = X_train_t,\n",
        "    t_c_train = X_train_t,\n",
        "    t_u_val = X_val_t,\n",
        "    t_c_val = X_val_t,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "SyXduYrFMn6H",
        "outputId": "404e182b-51a3-476a-b38d-297e5a6d666c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-dabd66d9b309>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mAdam1_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam1_learning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m training_loop(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam1_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-9c1680b7f1b7>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(n_epochs, optimizer, params, t_u_train, t_c_train, t_u_val, t_c_val)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mt_p_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_u_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_p_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_c_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mt_p_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_u_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-abc9ebde76fa>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(t_p, t_c)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#defining the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msquared_diffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_p\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msquared_diffs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (436) must match the size of tensor b (6) at non-singleton dimension 1"
          ]
        }
      ]
    }
  ]
}